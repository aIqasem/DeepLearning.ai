{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T23:20:17.595133Z",
     "iopub.status.busy": "2023-11-24T23:20:17.594785Z",
     "iopub.status.idle": "2023-11-24T23:20:17.627780Z",
     "shell.execute_reply": "2023-11-24T23:20:17.626874Z",
     "shell.execute_reply.started": "2023-11-24T23:20:17.595102Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T23:20:23.070283Z",
     "iopub.status.busy": "2023-11-24T23:20:23.069952Z",
     "iopub.status.idle": "2023-11-24T23:20:23.076885Z",
     "shell.execute_reply": "2023-11-24T23:20:23.075689Z",
     "shell.execute_reply.started": "2023-11-24T23:20:23.070260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cost function for logistic regression\n",
    "def compute_cost(X, y, w, b,  *argv):\n",
    "    m, n = X.shape\n",
    "    y_pred = sigmoid(np.dot(X,w) + b)\n",
    "    total_cost = (-1/m) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T23:20:29.466080Z",
     "iopub.status.busy": "2023-11-24T23:20:29.465613Z",
     "iopub.status.idle": "2023-11-24T23:20:29.472736Z",
     "shell.execute_reply": "2023-11-24T23:20:29.471682Z",
     "shell.execute_reply.started": "2023-11-24T23:20:29.466045Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gradient for logistic regression\n",
    "def compute_gradient(X, y, w, b, *argv): \n",
    "    m, n = X.shape\n",
    "    f_wb = sigmoid(np.dot(X, w) + b)\n",
    "    err = f_wb - y\n",
    "    dj_dw = np.dot(X.T, err) / m \n",
    "    dj_db = np.sum(err) / m \n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T23:20:37.842892Z",
     "iopub.status.busy": "2023-11-24T23:20:37.842544Z",
     "iopub.status.idle": "2023-11-24T23:20:37.850178Z",
     "shell.execute_reply": "2023-11-24T23:20:37.849098Z",
     "shell.execute_reply.started": "2023-11-24T23:20:37.842867Z"
    }
   },
   "outputs": [],
   "source": [
    "# Learning parameters using gradient descent\n",
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w_in = w_in - alpha * dj_dw               \n",
    "        b_in = b_in - alpha * dj_db              \n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T23:20:47.982551Z",
     "iopub.status.busy": "2023-11-24T23:20:47.982155Z",
     "iopub.status.idle": "2023-11-24T23:20:47.990518Z",
     "shell.execute_reply": "2023-11-24T23:20:47.989234Z",
     "shell.execute_reply.started": "2023-11-24T23:20:47.982521Z"
    }
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "\n",
    "    Returns:\n",
    "      p : (ndarray (m,)) The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "   \n",
    "    # Loop over each example\n",
    "    for i in range(m):   \n",
    "        z_wb = 0\n",
    "        \n",
    "        # Loop over each feature\n",
    "        for j in range(n): \n",
    "            # Add the corresponding term to z_wb\n",
    "            z_wb += X[i, j] * w[j]\n",
    "        \n",
    "        # Add bias term \n",
    "        z_wb += b\n",
    "        \n",
    "        # Calculate the prediction for this example\n",
    "        f_wb = 1 / (1 + np.exp(-z_wb))\n",
    "\n",
    "        # Apply the threshold\n",
    "        p[i] = 1 if f_wb >= 0.5 else 0\n",
    "        \n",
    "        return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T23:21:06.813932Z",
     "iopub.status.busy": "2023-11-24T23:21:06.813402Z",
     "iopub.status.idle": "2023-11-24T23:21:06.821343Z",
     "shell.execute_reply": "2023-11-24T23:21:06.820176Z",
     "shell.execute_reply.started": "2023-11-24T23:21:06.813884Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cost function for regularized logistic regression\n",
    "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "      lambda_ : (scalar, float) Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost : (scalar)     cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Calls the compute_cost function that you implemented above\n",
    "    cost_without_reg = compute_cost(X, y, w, b) \n",
    "    reg_cost = (lambda_ / (2 * m)) * np.sum(w**2)\n",
    "        \n",
    "    # Add the regularization cost to get the total cost\n",
    "    total_cost = cost_without_reg + reg_cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-24T23:21:13.445085Z",
     "iopub.status.busy": "2023-11-24T23:21:13.443885Z",
     "iopub.status.idle": "2023-11-24T23:21:13.452013Z",
     "shell.execute_reply": "2023-11-24T23:21:13.450129Z",
     "shell.execute_reply.started": "2023-11-24T23:21:13.445044Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gradient for regularized logistic regression\n",
    "def compute_gradient_reg(X, y, w, b, lambda_ = 1): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression with regularization\n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "      lambda_ : (scalar,float)  regularization constant\n",
    "    Returns\n",
    "      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b. \n",
    "      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. \n",
    "\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
    "    dj_dw = dj_dw + (lambda_ / m) * w        \n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
